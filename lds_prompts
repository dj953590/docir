Prompts for LDS Metadata
To design an effective prompt system for querying the `WL_DATA_CONCEPT` table in the lending domain, we’ll create **two interconnected prompts** that leverage natural language processing (NLP), structured data retrieval, and LLM-generated summaries. Below is the optimized framework:

---

### **Prompt System Design**

#### **1. Query Interpretation & Data Retrieval Prompt**
**Purpose**: Parse the natural language query, identify key terms, and retrieve relevant entries from `WL_DATA_CONCEPT`.  
**Prompt Template**:
```
You are a data concept expert for the lending domain in the WL_APP schema. Your task is to:
1. Analyze the user's query and extract key terms related to lending data concepts (e.g., "loan applications," "collateral," "credit scoring").
2. Map these terms to the closest matching entries in the WL_DATA_CONCEPT table (columns: table_name, description).
3. Return a JSON list of relevant table names and their descriptions.

**User Query**: "{user_query}"

**Instructions**:
- Prioritize tables where the description directly addresses the user's intent.
- If no direct match exists, suggest tables with related concepts (e.g., "credit risk" for "loan default prediction").
- Do not invent new concepts; use only the existing WL_DATA_CONCEPT entries.

**Output Format**:
{
  "query_keywords": ["keyword1", "keyword2"],
  "matched_concepts": [
    {"table_name": "TABLE_A", "description": "..."},
    {"table_name": "TABLE_B", "description": "..."}
  ]
}
```

---

#### **2. Summary & Insight Generation Prompt**
**Purpose**: Generate a user-friendly summary and insights using the retrieved data.  
**Prompt Template**:
```
You are a lending domain analyst. Use the provided data concepts from the WL_DATA_CONCEPT table to:
1. Summarize how these concepts address the user's query.
2. Add contextual insights about their role in lending workflows (e.g., "TABLE_X is critical for risk assessment").
3. Format the response clearly for non-technical users.

**User Query**: "{user_query}"  
**Matched Concepts**:  
{table_name: "TABLE_A", description: "..."},  
{table_name: "TABLE_B", description: "..."}

**Instructions**:
- Use bullet points for clarity.
- Highlight connections between concepts (e.g., "TABLE_A feeds data into TABLE_B").
- Keep technical jargon minimal but retain domain specificity (e.g., "APR" instead of "Annual Percentage Rate").

**Output Format**:
### Summary of Results for "{user_query}"
**Key Concepts**:  
- **TABLE_A**: [Description + LLM-generated insight, e.g., "Tracks loan applicant demographics; used to assess eligibility."]  
- **TABLE_B**: [Description + LLM-generated insight, e.g., "Stores collateral details; ensures loan security."]

**Insights**:  
- [LLM-generated analysis, e.g., "These tables are foundational for credit risk modeling."]
```

---

### **Example Workflow**
**User Query**:  
> "What data concepts are used to evaluate loan default risks?"

**Step 1**:  
- The **Query Interpretation Prompt** identifies keywords: `["loan default", "risk evaluation"]`.  
- Retrieves tables like `CREDIT_SCORING`, `LOAN_HISTORY`, and `COLLATERAL_DETAILS` from `WL_DATA_CONCEPT`.

**Step 2**:  
- The **Summary Prompt** generates:  
```
### Summary of Results for "loan default risks"
**Key Concepts**:  
- **CREDIT_SCORING**: "Tracks borrower credit scores; predicts likelihood of default based on historical data."  
- **LOAN_HISTORY**: "Stores past loan repayment behavior; identifies patterns in delinquencies."  

**Insights**:  
- These tables feed into risk models to flag high-risk applicants.  
- Combining credit scores and repayment history improves default prediction accuracy by 20%.
```

---

### **Key Features**
1. **Domain-Specific Filtering**: Ensures only lending-related concepts from `WL_DATA_CONCEPT` are used.  
2. **Hybrid Output**: Combines existing descriptions (for accuracy) with LLM insights (for clarity).  
3. **User-Centric Formatting**: Bullet points and plain language cater to both technical and non-technical users.  
4. **Error Handling**: Explicit instructions to avoid hallucinating new concepts.

---

### **Implementation Notes**
- Use **LangChain** or **LlamaIndex** to automate the retrieval-augmented generation (RAG) pipeline.  
- Store `WL_DATA_CONCEPT` in a vector database (e.g., Pinecone) for efficient semantic search.  
- Fine-tune the prompts with lending-specific examples (e.g., "mortgage underwriting") to improve accuracy.

This system ensures users get precise, actionable answers while leveraging the full context of the lending domain.


************************** Simple Approach ***************************
If we include the **entire `WL_DATA_CONCEPT` table data directly in the first prompt** (instead of retrieving it via external queries), the system flow changes significantly. Below is the revised architecture, along with trade-offs and implementation strategies:

---

### **Revised System Flow with Full Context**

#### **1. Modified Prompt Design**
**Prompt Template**:
```
You are a data expert for the lending domain. Below is a list of all data concepts (tables) in the WL_APP schema, stored in the WL_DATA_CONCEPT table:

{table_name: "LOAN_APPLICATIONS", description: "Tracks loan applicant details and application statuses."}
{table_name: "CREDIT_SCORING", description: "Stores credit scores and risk assessments for borrowers."}
{table_name: "COLLATERAL_DETAILS", description: "Records assets pledged as collateral for loans."}
... (all other rows from WL_DATA_CONCEPT)

**Task**:
1. Analyze the user's natural language query and identify relevant tables from the list above.
2. Return the matched table names, their descriptions, and a brief LLM-generated summary of how they address the query.

**User Query**: "{user_query}"

**Instructions**:
- Prioritize tables whose descriptions directly relate to the query.
- Do not invent new tables or descriptions.
- Use the format:
  **Matched Tables**:
  - [Table Name]: [Description] → [LLM Summary]
```

---

#### **2. Updated Workflow**
1. **Preprocessing**:
   - Extract all rows from `WL_DATA_CONCEPT` and format them as a list of `{table_name, description}` pairs.
   - Inject this list into the prompt template.

2. **Query Execution**:
   - Send the entire prompt (with the full `WL_DATA_CONCEPT` data) to the LLM.
   - The LLM directly processes the user’s query against the provided table data.

3. **Response Generation**:
   - The LLM identifies relevant tables, uses their descriptions, and generates summaries.

**Example**:
- **User Query**: "Which tables track borrower risk?"
- **LLM Output**:
  ```
  **Matched Tables**:
  - CREDIT_SCORING: "Stores credit scores and risk assessments for borrowers." → Provides metrics like credit scores and debt-to-income ratios to evaluate borrower risk.
  - LOAN_DEFAULT_HISTORY: "Records historical loan defaults." → Identifies patterns in past defaults to predict future risks.
  ```

---

### **Key Changes & Trade-offs**

| **Aspect**               | **Original Flow**                          | **New Flow (Full Context)**                |
|--------------------------|--------------------------------------------|--------------------------------------------|
| **Data Integration**      | External database/vector search.           | All data embedded in the prompt.           |
| **Token Usage**           | Low (only keywords sent).                  | High (entire table data sent).             |
| **Accuracy**              | Depends on search algorithm.               | Depends on LLM’s ability to parse context. |
| **Scalability**           | Handles large tables.                      | Limited by LLM’s context window.           |
| **Implementation Complexity** | Requires DB/vector DB setup.         | Simple (no external systems).              |

---

### **When to Use This Approach**
1. **Small Tables**: If `WL_DATA_CONCEPT` has fewer than 50 rows (or fits within the LLM’s token limit).
2. **Static Data**: When table descriptions rarely change (avoids frequent prompt updates).
3. **Simplified Systems**: For prototypes or environments without database infrastructure.

---

### **Implementation Code (Python)**
```python
from typing import List, Dict
import json

def build_prompt(user_query: str, wl_data_concept: List[Dict]) -> str:
    # Format WL_DATA_CONCEPT entries as strings
    concept_entries = [
        f'{{table_name: "{entry["table_name"]}", description: "{entry["description"]}"}}'
        for entry in wl_data_concept
    ]
    concepts_str = "\n".join(concept_entries)
    
    prompt = f"""
    You are a data expert for the lending domain. Below is a list of all data concepts (tables) in the WL_APP schema:

    {concepts_str}

    Task:
    1. Analyze the user's query and identify relevant tables.
    2. Return matched tables with descriptions and LLM-generated summaries.

    User Query: "{user_query}"

    Instructions:
    - Use only the provided table data.
    - Format your response as:
    **Matched Tables**:
    - [Table Name]: [Description] → [Summary]
    """
    return prompt

# Example usage
wl_data_concept = [
    {"table_name": "LOAN_APPLICATIONS", "description": "Tracks loan applicant details."},
    {"table_name": "CREDIT_SCORING", "description": "Stores borrower credit scores."}
]

user_query = "What tables track borrower eligibility?"
prompt = build_prompt(user_query, wl_data_concept)
response = query_llm(prompt)  # Assume query_llm() calls the LLM API
print(response)
```

---

### **Challenges & Mitigations**
1. **Token Limits**:
   - Use compression: Shorten descriptions (e.g., "Tracks loan apps" instead of "Tracks loan applicant details and application statuses").
   - Switch to a higher-context model (e.g., GPT-4 Turbo with 128k tokens).

2. **Accuracy**:
   - Add **emphasis tags** to critical terms (e.g., `Stores <risk> assessments`).
   - Include **aliases** for table names (e.g., "CREDIT_SCORING (aka Risk Scores)").

3. **Cost**:
   - Cache frequent queries to avoid reprocessing the full context repeatedly.

---

### **Comparison of Approaches**

| **Criteria**              | **Original Flow**               | **Full-Context Flow**           |
|---------------------------|----------------------------------|----------------------------------|
| Data Handling              | Dynamic retrieval               | Static embedding in prompt      |
| Token Efficiency           | ✅ Optimal                      | ❌ High                         |
| Suitability for Large Data | ✅ Yes                          | ❌ No (context limits)          |
| Real-Time Updates          | ✅ Yes (DB updates apply)       | ❌ No (requires prompt rebuild) |
| Latency                    | ❌ Higher (DB + LLM calls)      | ✅ Lower (single LLM call)      |

---

### **Recommendation**
- Use the **full-context flow** only if:
  - The `WL_DATA_CONCEPT` table is small (< 50 rows).
  - You prioritize simplicity over scalability.
  - Descriptions are concise and unlikely to change.
- Otherwise, stick with the **original retrieval-augmented flow** for better scalability and accuracy.

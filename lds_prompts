Prompts for LDS Metadata
To design an effective prompt system for querying the `WL_DATA_CONCEPT` table in the lending domain, we’ll create **two interconnected prompts** that leverage natural language processing (NLP), structured data retrieval, and LLM-generated summaries. Below is the optimized framework:

---

### **Prompt System Design**

#### **1. Query Interpretation & Data Retrieval Prompt**
**Purpose**: Parse the natural language query, identify key terms, and retrieve relevant entries from `WL_DATA_CONCEPT`.  
**Prompt Template**:
```
You are a data concept expert for the lending domain in the WL_APP schema. Your task is to:
1. Analyze the user's query and extract key terms related to lending data concepts (e.g., "loan applications," "collateral," "credit scoring").
2. Map these terms to the closest matching entries in the WL_DATA_CONCEPT table (columns: table_name, description).
3. Return a JSON list of relevant table names and their descriptions.

**User Query**: "{user_query}"

**Instructions**:
- Prioritize tables where the description directly addresses the user's intent.
- If no direct match exists, suggest tables with related concepts (e.g., "credit risk" for "loan default prediction").
- Do not invent new concepts; use only the existing WL_DATA_CONCEPT entries.

**Output Format**:
{
  "query_keywords": ["keyword1", "keyword2"],
  "matched_concepts": [
    {"table_name": "TABLE_A", "description": "..."},
    {"table_name": "TABLE_B", "description": "..."}
  ]
}
```

---

#### **2. Summary & Insight Generation Prompt**
**Purpose**: Generate a user-friendly summary and insights using the retrieved data.  
**Prompt Template**:
```
You are a lending domain analyst. Use the provided data concepts from the WL_DATA_CONCEPT table to:
1. Summarize how these concepts address the user's query.
2. Add contextual insights about their role in lending workflows (e.g., "TABLE_X is critical for risk assessment").
3. Format the response clearly for non-technical users.

**User Query**: "{user_query}"  
**Matched Concepts**:  
{table_name: "TABLE_A", description: "..."},  
{table_name: "TABLE_B", description: "..."}

**Instructions**:
- Use bullet points for clarity.
- Highlight connections between concepts (e.g., "TABLE_A feeds data into TABLE_B").
- Keep technical jargon minimal but retain domain specificity (e.g., "APR" instead of "Annual Percentage Rate").

**Output Format**:
### Summary of Results for "{user_query}"
**Key Concepts**:  
- **TABLE_A**: [Description + LLM-generated insight, e.g., "Tracks loan applicant demographics; used to assess eligibility."]  
- **TABLE_B**: [Description + LLM-generated insight, e.g., "Stores collateral details; ensures loan security."]

**Insights**:  
- [LLM-generated analysis, e.g., "These tables are foundational for credit risk modeling."]
```

---

### **Example Workflow**
**User Query**:  
> "What data concepts are used to evaluate loan default risks?"

**Step 1**:  
- The **Query Interpretation Prompt** identifies keywords: `["loan default", "risk evaluation"]`.  
- Retrieves tables like `CREDIT_SCORING`, `LOAN_HISTORY`, and `COLLATERAL_DETAILS` from `WL_DATA_CONCEPT`.

**Step 2**:  
- The **Summary Prompt** generates:  
```
### Summary of Results for "loan default risks"
**Key Concepts**:  
- **CREDIT_SCORING**: "Tracks borrower credit scores; predicts likelihood of default based on historical data."  
- **LOAN_HISTORY**: "Stores past loan repayment behavior; identifies patterns in delinquencies."  

**Insights**:  
- These tables feed into risk models to flag high-risk applicants.  
- Combining credit scores and repayment history improves default prediction accuracy by 20%.
```

---

### **Key Features**
1. **Domain-Specific Filtering**: Ensures only lending-related concepts from `WL_DATA_CONCEPT` are used.  
2. **Hybrid Output**: Combines existing descriptions (for accuracy) with LLM insights (for clarity).  
3. **User-Centric Formatting**: Bullet points and plain language cater to both technical and non-technical users.  
4. **Error Handling**: Explicit instructions to avoid hallucinating new concepts.

---

### **Implementation Notes**
- Use **LangChain** or **LlamaIndex** to automate the retrieval-augmented generation (RAG) pipeline.  
- Store `WL_DATA_CONCEPT` in a vector database (e.g., Pinecone) for efficient semantic search.  
- Fine-tune the prompts with lending-specific examples (e.g., "mortgage underwriting") to improve accuracy.

This system ensures users get precise, actionable answers while leveraging the full context of the lending domain.


************************** Simple Approach ***************************
If we include the **entire `WL_DATA_CONCEPT` table data directly in the first prompt** (instead of retrieving it via external queries), the system flow changes significantly. Below is the revised architecture, along with trade-offs and implementation strategies:

---

### **Revised System Flow with Full Context**

#### **1. Modified Prompt Design**
**Prompt Template**:
```
You are a data expert for the lending domain. Below is a list of all data concepts (tables) in the WL_APP schema, stored in the WL_DATA_CONCEPT table:

{table_name: "LOAN_APPLICATIONS", description: "Tracks loan applicant details and application statuses."}
{table_name: "CREDIT_SCORING", description: "Stores credit scores and risk assessments for borrowers."}
{table_name: "COLLATERAL_DETAILS", description: "Records assets pledged as collateral for loans."}
... (all other rows from WL_DATA_CONCEPT)

**Task**:
1. Analyze the user's natural language query and identify relevant tables from the list above.
2. Return the matched table names, their descriptions, and a brief LLM-generated summary of how they address the query.

**User Query**: "{user_query}"

**Instructions**:
- Prioritize tables whose descriptions directly relate to the query.
- Do not invent new tables or descriptions.
- Use the format:
  **Matched Tables**:
  - [Table Name]: [Description] → [LLM Summary]
```

---

#### **2. Updated Workflow**
1. **Preprocessing**:
   - Extract all rows from `WL_DATA_CONCEPT` and format them as a list of `{table_name, description}` pairs.
   - Inject this list into the prompt template.

2. **Query Execution**:
   - Send the entire prompt (with the full `WL_DATA_CONCEPT` data) to the LLM.
   - The LLM directly processes the user’s query against the provided table data.

3. **Response Generation**:
   - The LLM identifies relevant tables, uses their descriptions, and generates summaries.

**Example**:
- **User Query**: "Which tables track borrower risk?"
- **LLM Output**:
  ```
  **Matched Tables**:
  - CREDIT_SCORING: "Stores credit scores and risk assessments for borrowers." → Provides metrics like credit scores and debt-to-income ratios to evaluate borrower risk.
  - LOAN_DEFAULT_HISTORY: "Records historical loan defaults." → Identifies patterns in past defaults to predict future risks.
  ```

---

### **Key Changes & Trade-offs**

| **Aspect**               | **Original Flow**                          | **New Flow (Full Context)**                |
|--------------------------|--------------------------------------------|--------------------------------------------|
| **Data Integration**      | External database/vector search.           | All data embedded in the prompt.           |
| **Token Usage**           | Low (only keywords sent).                  | High (entire table data sent).             |
| **Accuracy**              | Depends on search algorithm.               | Depends on LLM’s ability to parse context. |
| **Scalability**           | Handles large tables.                      | Limited by LLM’s context window.           |
| **Implementation Complexity** | Requires DB/vector DB setup.         | Simple (no external systems).              |

---

### **When to Use This Approach**
1. **Small Tables**: If `WL_DATA_CONCEPT` has fewer than 50 rows (or fits within the LLM’s token limit).
2. **Static Data**: When table descriptions rarely change (avoids frequent prompt updates).
3. **Simplified Systems**: For prototypes or environments without database infrastructure.

---

### **Implementation Code (Python)**
```python
from typing import List, Dict
import json

def build_prompt(user_query: str, wl_data_concept: List[Dict]) -> str:
    # Format WL_DATA_CONCEPT entries as strings
    concept_entries = [
        f'{{table_name: "{entry["table_name"]}", description: "{entry["description"]}"}}'
        for entry in wl_data_concept
    ]
    concepts_str = "\n".join(concept_entries)
    
    prompt = f"""
    You are a data expert for the lending domain. Below is a list of all data concepts (tables) in the WL_APP schema:

    {concepts_str}

    Task:
    1. Analyze the user's query and identify relevant tables.
    2. Return matched tables with descriptions and LLM-generated summaries.

    User Query: "{user_query}"

    Instructions:
    - Use only the provided table data.
    - Format your response as:
    **Matched Tables**:
    - [Table Name]: [Description] → [Summary]
    """
    return prompt

# Example usage
wl_data_concept = [
    {"table_name": "LOAN_APPLICATIONS", "description": "Tracks loan applicant details."},
    {"table_name": "CREDIT_SCORING", "description": "Stores borrower credit scores."}
]

user_query = "What tables track borrower eligibility?"
prompt = build_prompt(user_query, wl_data_concept)
response = query_llm(prompt)  # Assume query_llm() calls the LLM API
print(response)
```

---

### **Challenges & Mitigations**
1. **Token Limits**:
   - Use compression: Shorten descriptions (e.g., "Tracks loan apps" instead of "Tracks loan applicant details and application statuses").
   - Switch to a higher-context model (e.g., GPT-4 Turbo with 128k tokens).

2. **Accuracy**:
   - Add **emphasis tags** to critical terms (e.g., `Stores <risk> assessments`).
   - Include **aliases** for table names (e.g., "CREDIT_SCORING (aka Risk Scores)").

3. **Cost**:
   - Cache frequent queries to avoid reprocessing the full context repeatedly.

---

### **Comparison of Approaches**

| **Criteria**              | **Original Flow**               | **Full-Context Flow**           |
|---------------------------|----------------------------------|----------------------------------|
| Data Handling              | Dynamic retrieval               | Static embedding in prompt      |
| Token Efficiency           | ✅ Optimal                      | ❌ High                         |
| Suitability for Large Data | ✅ Yes                          | ❌ No (context limits)          |
| Real-Time Updates          | ✅ Yes (DB updates apply)       | ❌ No (requires prompt rebuild) |
| Latency                    | ❌ Higher (DB + LLM calls)      | ✅ Lower (single LLM call)      |

---

### **Recommendation**
- Use the **full-context flow** only if:
  - The `WL_DATA_CONCEPT` table is small (< 50 rows).
  - You prioritize simplicity over scalability.
  - Descriptions are concise and unlikely to change.
- Otherwise, stick with the **original retrieval-augmented flow** for better scalability and accuracy.

Here's a Python implementation that integrates with the HDB class, retrieves WL_DATA_CONCEPT data, constructs prompts, and processes natural language queries:

```python
from hdb import HDB  # Assuming HDB provides database connection
from typing import List, Dict
import json

class ConceptQueryProcessor:
    def __init__(self):
        self.hdb_connection = HDB().get_connection()
        self.concept_data = self._load_concept_data()
    
    def _load_concept_data(self) -> List[Dict]:
        """Fetch and format WL_DATA_CONCEPT table data"""
        query = """
            SELECT "Concept Name", "Table Name", "Description"
            FROM WL_APP.WL_DATA_CONCEPT
        """
        
        with self.hdb_connection.cursor() as cursor:
            cursor.execute(query)
            columns = [col[0] for col in cursor.description]
            return [dict(zip(columns, row)) for row in cursor.fetchall()]
    
    def _build_prompt(self, user_query: str) -> str:
        """Construct LLM prompt with concept data"""
        concept_entries = "\n".join(
            f'Concept: {row["Concept Name"]} | Table: {row["Table Name"]} | Description: {row["Description"]}'
            for row in self.concept_data
        )
        
        return f"""
        You are a lending domain data expert. Use the following concepts to answer the query:
        {concept_entries}
        
        Query: {user_query}
        
        Format response as:
        {{
            "summary": "concise summary of how concepts address the query",
            "matched_concepts": [
                {{
                    "concept": "Concept Name",
                    "table": "Table Name",
                    "description": "Original Description",
                    "relevance": "LLM-generated relevance explanation"
                }}
            ]
        }}
        """
    
    def process_query(self, user_query: str) -> dict:
        """Execute full query processing pipeline"""
        if not self.concept_data:
            raise ValueError("No concept data loaded")
            
        prompt = self._build_prompt(user_query)
        llm_response = self._query_llm(prompt)
        return self._parse_response(llm_response)
    
    def _query_llm(self, prompt: str) -> str:
        """Execute LLM query (mock implementation)"""
        # Replace with actual LLM API call
        # Example using OpenAI:
        from openai import OpenAI
        client = OpenAI()
        
        response = client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        return response.choices[0].message.content
    
    def _parse_response(self, response: str) -> dict:
        """Parse and validate LLM response"""
        try:
            data = json.loads(response)
            # Add validation checks here
            return data
        except json.JSONDecodeError:
            return {"error": "Failed to parse LLM response"}

# Example usage
if __name__ == "__main__":
    processor = ConceptQueryProcessor()
    
    user_query = "Which concepts track loan application status and risk assessment?"
    result = processor.process_query(user_query)
    
    print("Summary:", result.get("summary"))
    print("\nMatched Concepts:")
    for concept in result.get("matched_concepts", []):
        print(f"• {concept['concept']} ({concept['table']})")
        print(f"  Relevance: {concept['relevance']}")
        print(f"  Description: {concept['description']}\n")
```

### Key Components:

1. **HDB Integration**:
```python
self.hdb_connection = HDB().get_connection()
with self.hdb_connection.cursor() as cursor:
    cursor.execute(query)
```

2. **Prompt Construction**:
```python
concept_entries = "\n".join(
    f'Concept: {row["Concept Name"]} | Table: {row["Table Name"]} | Description: {row["Description"]}'
    for row in self.concept_data
)
```

3. **Response Format**:
```json
{
    "summary": "...",
    "matched_concepts": [
        {
            "concept": "Loan Applications",
            "table": "LOAN_APP",
            "description": "Tracks loan application lifecycle",
            "relevance": "Directly tracks application status changes"
        }
    ]
}
```

### Features:
1. **Automatic Data Loading**: Fetches latest concept data on initialization
2. **Structured Output**: Enforces consistent JSON response format
3. **Context Preservation**: Maintains original descriptions while adding LLM insights
4. **Error Handling**: Gracefully handles empty data and parsing errors

### Example Output:
```
Summary: Loan application tracking and risk assessment are covered by these core concepts

Matched Concepts:
• Loan Applications (LOAN_APP)
  Relevance: Directly tracks application status changes
  Description: Tracks loan application lifecycle from submission to decision

• Credit Risk Model (CREDIT_RISK)
  Relevance: Provides risk scoring for applicant evaluation
  Description: Machine learning model for default probability prediction
```

### To Improve:
1. Add pagination if WL_DATA_CONCEPT exceeds LLM context limits
2. Implement result caching for frequent queries
3. Add schema validation for LLM responses
4. Include fallback mechanisms for failed LLM responses

Let me know if you need modifications for:
1. Specific HDB connection parameters
2. Different LLM providers
3. Additional data validation rules
4. Performance optimizations for large datasets

Here's an enhanced version of the `HDB` class using Pydantic's `BaseModel` for validation and configuration management:

```python
from pydantic import BaseModel, Field, validator, PrivateAttr
from sqlalchemy import create_engine, event, text
from sqlalchemy.orm import sessionmaker, scoped_session
from typing import Optional, Generator
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DBConfig(BaseModel):
    """Pydantic model for database connection configuration"""
    username: str = Field(..., description="Database username")
    password: str = Field(..., description="Database password")
    host: str = Field("localhost", description="Database host")
    port: str = Field("1521", description="Database port")
    service_name: str = Field(..., description="Oracle service name")
    schema_name: str = Field(..., alias="schema", description="Target schema name")

    @validator('schema_name', pre=True)
    def uppercase_schema(cls, v):
        """Ensure schema name is uppercase"""
        return v.upper()

class HDB(BaseModel):
    """Database handler with schema management using Pydantic"""
    config: DBConfig
    _engine: Optional[create_engine] = PrivateAttr(None)
    _session_factory: Optional[sessionmaker] = PrivateAttr(None)

    class Config:
        arbitrary_types_allowed = True
        underscore_attrs_are_private = True

    def connect(self) -> None:
        """Establish database connection with schema configuration"""
        conn_str = (
            f"oracle+cx_oracle://{self.config.username}:{self.config.password}"
            f"@{self.config.host}:{self.config.port}/?service_name={self.config.service_name}"
        )

        self._engine = create_engine(
            conn_str,
            pool_size=5,
            max_overflow=10,
            pool_pre_ping=True,
            echo=False
        )

        # Set default schema for all connections
        @event.listens_for(self._engine, "connect", insert=True)
        def set_current_schema(dbapi_connection, connection_record):
            with dbapi_connection.cursor() as cursor:
                cursor.execute(f"ALTER SESSION SET CURRENT_SCHEMA = {self.config.schema_name}")
            logger.info(f"Set schema to {self.config.schema_name}")

        self._session_factory = scoped_session(
            sessionmaker(
                bind=self._engine,
                autocommit=False,
                autoflush=False
            )
        )

    def get_session(self) -> Generator:
        """Get a thread-safe database session"""
        if not self._session_factory:
            raise ConnectionError("Database not connected. Call connect() first")
        
        session = self._session_factory()
        try:
            yield session
            session.commit()
        except Exception as e:
            session.rollback()
            logger.error(f"Session rollback due to error: {str(e)}")
            raise
        finally:
            session.close()

    def check_connection(self) -> bool:
        """Verify schema connection"""
        try:
            with self._engine.connect() as conn:
                result = conn.execute(
                    text("SELECT SYS_CONTEXT('USERENV', 'CURRENT_SCHEMA') FROM DUAL")
                )
                current_schema = result.scalar()
                logger.info(f"Current schema: {current_schema}")
                return current_schema == self.config.schema_name
        except Exception as e:
            logger.error(f"Connection check failed: {str(e)}")
            return False

    @property
    def is_connected(self) -> bool:
        """Connection status check"""
        return self._engine is not None and self.check_connection()

    def __enter__(self):
        self.connect()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.disconnect()

    def disconnect(self) -> None:
        """Clean up database connections"""
        if self._engine:
            self._engine.dispose()
            logger.info("Database connection pool closed")

# Usage Example ###########################################################
if __name__ == "__main__":
    # Initialize with validated configuration
    db_config = DBConfig(
        username="app_user",
        password="secure_password",
        service_name="ORCL",
        schema="wl_app"  # Will be converted to WL_APP
    )

    with HDB(config=db_config) as hdb:
        # Check connection status
        if not hdb.is_connected:
            raise RuntimeError("Failed to connect to database")
        
        # Execute query using context manager
        with hdb.get_session() as session:
            result = session.execute(
                text("""
                    SELECT "Concept Name", "Table Name", "Description"
                    FROM WL_DATA_CONCEPT
                """)
            )
            
            concepts = [dict(row) for row in result]
            print(f"Found {len(concepts)} data concepts in schema {db_config.schema_name}")
```

### Key Improvements:
1. **Pydantic Validation**:
   - Schema name auto-conversion to uppercase
   - Type checking for all connection parameters
   - Config validation using `@validator` decorator

2. **Connection Management**:
   - Context manager support (`with` statements)
   - Connection pooling with SQLAlchemy
   - Automatic schema setting on connect

3. **Error Handling**:
   - Session rollback on errors
   - Connection health checks
   - Proper connection cleanup

4. **Additional Features**:
   - Thread-safe scoped sessions
   - Connection pool configuration
   - Logging integration
   - Type hints throughout

### Usage Pattern:
```python
# 1. Create validated configuration
config = DBConfig(
    username="user",
    password="pass",
    service_name="ORCL",
    schema="wl_app"
)

# 2. Initialize and connect
hdb = HDB(config=config)
hdb.connect()

# 3. Execute queries
with hdb.get_session() as session:
    session.execute(text("..."))
```

This implementation ensures:
- Strong type safety with Pydantic
- Proper schema management
- Connection pooling and cleanup
- Thread-safe operations
- Comprehensive logging and error handling

Let me know if you need any modifications to better match your specific Oracle environment!
